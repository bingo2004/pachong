# 个人爬虫总记

## 前言

**个人python之路的起点．**

从网络上最普及的爬取豆瓣高分电影开始，从手抄别人的代码起步，逐步学习和吸收，完成了一个系列网站的自动爬取．包括：水木十大，懂球帝头条，糗事百科高赞数的段子以及一些关注的博客．部署到服务器上，每日自动运行一次并邮件发送到手机上．

本仓库内包括了我爬虫学习之路

## 豆瓣电影

功能是爬取了豆瓣高分电影前250名．这是我成功运行的第一个爬虫程序，写在这里聊以纪念．
这段代码是抄袭的，我就没有放到这个仓库里．

## 水木

这是个人登录最频繁的网站，这里的功能是抓取当日十大的title和网址，并将正文列出前200个字符，如果需要细看也可以点击网址．

## 糗事百科高点赞数的段子

抓取的内容为糗事百科文字版，首页内点赞数超过3000的段子．记录下段子内容和点赞数

## 懂球帝头条

只获取了首页置顶的５则新闻，足够每天的信息量．

## 知乎日报

获取了日报首页前５则，列出名称和具体地址，如果感兴趣可以点击网址进入细看．

## 博客

月光博客，每日自动进入网站查看一次，如果本日有更新，则抓取更新内容．

## 1024

~~**该内容不可描述**~~

A的作用是打开1024图片主页的每一个链接，并获取每个图片页内的链接，保存为url.txt文件

B的作用是逐行读取url文件，若为图片链接则下载

分成AB两部分的思路是因为希望代理完成Ａ，非代理完成B，提高图片爬取速度．后面发现非代理只能爬取一部分图片链接，这是后话．

## 邮件发送

顺便学习了如何使用smtplib.
*sendmail0.py* 为发送普通邮件．　*sendmail.py* 为发送带附件的邮件．
